{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "load_trt_plan_file_resnet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# coding: utf-8",
      "executable": "/usr/bin/env python",
      "notebook_metadata_filter": "-all",
      "text_representation": {
        "extension": ".py",
        "format_name": "light"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanochk/myRepo/blob/master/load_trt_plan_file_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFOB-3SlV7OR"
      },
      "source": [
        "# Load and infer a TRT plan file from a TF-TRT converted model (TF2)\n",
        "This notebook demonstrates how to load a TensorRT plan file from a TF-TRT converted model and run TRT inference with the plan file. The plan file is a serialized TRT engine.\n",
        "\n",
        "**Note:** This example assumes that the whole model is converted to a single TRT engine. In that case the single plan file can be used to infer the model using TRT's C++ or Python API.\n",
        "\n",
        "To run this notebook, a recent TF version is necessary, for example the [NGC docker container](https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow) nvcr.io/nvidia/tensorflow:20.12-tf2-py3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO7HjfwwV7OT"
      },
      "source": [
        "# Verbose debug output about the TF-TRT conversion\n",
        "import os\n",
        "os.environ[\"TF_CPP_VMODULE\"]=\"convert_graph=1,segment=1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts5EMZZnh6uw"
      },
      "source": [
        "Might need to install some packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5GhqC9nh6ux"
      },
      "source": [
        "!pip install pillow matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81bPHMwoV7OX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "import numpy as np\n",
        "from tensorflow.python.saved_model import signature_constants\n",
        "from tensorflow.python.saved_model import tag_constants\n",
        "from tensorflow.python.framework import convert_to_constants\n",
        "from tensorflow.python.ops import gen_math_ops\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poxrUKgFV7Ob"
      },
      "source": [
        "## 1. Load and save the TF2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYBvdh7vV7Oc"
      },
      "source": [
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
        "\n",
        "model = ResNet50V2(weights='imagenet') \n",
        "tf.saved_model.save(model, 'resnet_v2_50_saved_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge5q1_VXh6uy"
      },
      "source": [
        "Define model input parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR6dEEHoh6uy"
      },
      "source": [
        "img_shape = [224, 224, 3]\n",
        "batch_size = 1\n",
        "input_shape = [batch_size,] + img_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QqJoDpqh6uy"
      },
      "source": [
        "### 1.1 Get input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97DuMshvh6uz"
      },
      "source": [
        "!mkdir data\n",
        "!wget  -O ./data/img0.JPG \"https://d17fnq9dkz9hgj.cloudfront.net/breed-uploads/2018/08/siberian-husky-detail.jpg?bust=1535566590&width=630\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IEzO-qFh6uz"
      },
      "source": [
        "img_path = './data/img0.JPG'\n",
        "input_img = image.load_img(img_path, target_size=(224, 224))\n",
        "plt.imshow(input_img);\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sH4rvTJh6uz"
      },
      "source": [
        "Preprocess the data for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zud0V43Gh6uz"
      },
      "source": [
        "input_x = image.img_to_array(input_img)\n",
        "input_x = np.expand_dims(input_x, axis=0)\n",
        "input_x = preprocess_input(input_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SCJac_9V7Oj"
      },
      "source": [
        "## 2. Convert model to TF-TRT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4UG3chmV7Oj"
      },
      "source": [
        "conv_params=trt.TrtConversionParams(precision_mode='FP16', max_workspace_size_bytes=1<<30)\n",
        "converter = trt.TrtGraphConverterV2(input_saved_model_dir=\"resnet_v2_50_saved_model\", conversion_params=conv_params)\n",
        "converter.convert()\n",
        "\n",
        "def input_fn():\n",
        "    input_shapes = [[input_x.shape], ]       \n",
        "    for shapes in input_shapes:\n",
        "        # return a list of input tensors\n",
        "        yield [np.ones(shape=x).astype(np.float32) for x in shapes]\n",
        "        \n",
        "converter.build(input_fn)\n",
        "converter.save(\"resnet_v2_50_trt\")\n",
        "print(\"Model exported to TRT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2BVT9cOh6u1"
      },
      "source": [
        "### 2.1 Load and test converted model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMTEpkShh6u1"
      },
      "source": [
        "def get_func_from_saved_model(saved_model_dir):\n",
        "    saved_model_loaded = tf.saved_model.load(\n",
        "        saved_model_dir, tags=[tag_constants.SERVING])\n",
        "    graph_func = saved_model_loaded.signatures[\n",
        "        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
        "    return graph_func, saved_model_loaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPap2cXsh6u1"
      },
      "source": [
        "trt_func, _ = get_func_from_saved_model('resnet_v2_50_trt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPc1iwjCh6u1"
      },
      "source": [
        "Run prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJb2tD_Bh6u1"
      },
      "source": [
        "preds = trt_func(tf.convert_to_tensor(input_x))['predictions'].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW8bdx4Qh6u2"
      },
      "source": [
        "The tensor `preds` stores the predicted class probabilities for the single input image in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FC6y3lph6u2"
      },
      "source": [
        "decode_predictions(preds, top=3)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciBXNPr1V7Or"
      },
      "source": [
        "## 3. Load engine file (plan file) directly to TensorRT\n",
        "The converted model is saved under `resnet_v2_50_trt`. The TensorRT engines can be found in the `assets` subdirectory. Each TRTEngineOp has a separate asset file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nKGPYciV7Os"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# This is the protobuf that is needed to load the TF plan file\n",
        "from tensorflow.compiler.tf2tensorrt.utils.trt_engine_instance_pb2 import TRTEngineInstance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvb0zFhaV7Ov"
      },
      "source": [
        "# Check if the file is where we expect it to be\n",
        "!ls -lh resnet_v2_50_trt/assets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGPEhksAV7Oy"
      },
      "source": [
        "asset_file = 'resnet_v2_50_trt/assets/trt-serialized-engine.TRTEngineOp_0_0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzXqn6nqV7O1"
      },
      "source": [
        "### 3.1 Load engine file (plan file)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUU7Q9mnV7O2"
      },
      "source": [
        "def read_plan_from_asset(asset_file):\n",
        "    \"\"\" Read the serialized TRT engine from an asset file, and return the buffer that contains the first engine.\n",
        "    Arguments: asset_file -- path to the file\n",
        "    \"\"\"\n",
        "    raw_dataset = tf.data.TFRecordDataset([asset_file])\n",
        "\n",
        "    # Note that the asset file could contain multiple engines, one for each input shape that the\n",
        "    # corresponding TRTEngineOp handles. Here we only return the buffer that contains the first engine.\n",
        "    for raw_record in raw_dataset.take(1):\n",
        "        engine_instance = TRTEngineInstance()\n",
        "        engine_instance.ParseFromString(raw_record.numpy())\n",
        "        # print(\"Loaded engine for shape\", engine_instance.input_shapes)\n",
        "        \n",
        "    return engine_instance.serialized_engine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtwAZmimV7O4"
      },
      "source": [
        "engine_string = read_plan_from_asset(asset_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oooTcXIsh6u4"
      },
      "source": [
        "### 3.2 Deserialize the engine and inspect it\n",
        "First we load the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeVW3Fp3h6u4"
      },
      "source": [
        "!pip install pycuda # can take a few minutes\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import tensorrt as trt\n",
        "logger = trt.Logger()\n",
        "runtime = trt.Runtime(logger)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8dm__zth6u4"
      },
      "source": [
        "with runtime.deserialize_cuda_engine(engine_string) as engine:\n",
        "    print('Engine bindings:')\n",
        "    for i in range(engine.num_bindings):\n",
        "        print('Name:',engine[i], ', shape: ', engine.get_binding_shape(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG3y0Yk8h6u4"
      },
      "source": [
        "Since the whole model was converted to this engine, the input and output bindings correspond to the TF model's input and output tensors. If the binding shapes do not match the TF model input/output shapes, then the engine contains only a part of the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DQknlLrV7O8"
      },
      "source": [
        "### 3.3 Write the engine file into a standalone binary file\n",
        "In case someone prefers plain binary files. This is not used in this notebook, just provided as reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqydYewVV7O9"
      },
      "source": [
        "with open(\"/tmp/plan_file.trt\",\"wb\") as f:\n",
        "    f.write(engine_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfmXwfSbV7PA"
      },
      "source": [
        "## 4 Native TensorRT inference in Python\n",
        "Define helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcMf0ZeXV7PE"
      },
      "source": [
        "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
        "class HostDeviceMem(object):\n",
        "    def __init__(self, host_mem, device_mem):\n",
        "        self.host = host_mem\n",
        "        self.device = device_mem\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "    \n",
        "# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
        "def allocate_buffers(engine):\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    bindings = []\n",
        "    stream = cuda.Stream()\n",
        "    for binding in engine:\n",
        "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
        "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
        "        # Allocate host and device buffers\n",
        "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
        "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
        "        # Append the device buffer to device bindings.\n",
        "        bindings.append(int(device_mem))\n",
        "        # Append to the appropriate list.\n",
        "        if engine.binding_is_input(binding):\n",
        "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "        else:\n",
        "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "    return inputs, outputs, bindings, stream\n",
        "\n",
        "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
        "    # Transfer input data to the GPU.\n",
        "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
        "    # Run inference.\n",
        "    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n",
        "    # Transfer predictions back from the GPU.\n",
        "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
        "    # Synchronize the stream\n",
        "    stream.synchronize()\n",
        "    # Return only the host outputs.\n",
        "    return [out.host for out in outputs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBuOaNCWV7PH"
      },
      "source": [
        "Run inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhG3SMXdV7PH"
      },
      "source": [
        "with runtime.deserialize_cuda_engine(engine_string) as engine:\n",
        "    allocate_buffers(engine)\n",
        "    inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
        "    with engine.create_execution_context() as context:\n",
        "        # Set input data\n",
        "        np.copyto(inputs[0].host, input_x.ravel())\n",
        "        [output] = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAJ4NdgoV7PK"
      },
      "source": [
        "output = output.reshape(batch_size, len(output)//batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulY8Fn74h6u7"
      },
      "source": [
        "decode_predictions(output, top=3)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrPod8nCh6u7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}